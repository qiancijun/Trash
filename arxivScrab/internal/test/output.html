<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Document</title>
</head>
<body>
    <table>
        
            <tr class="item">
                
                <td><a href="https://arxiv.org/abs/2406.02514">arXiv:2406.02514</a></td>
                <td>Approximate path decompositions of regular graphs</td>
                <td>Richard Montgomery, Alp Müyesser, Alexey Pokrovskiy, Benny Sudakov</td>
                <td>We show that the edges of any $d$-regular graph can be almost decomposed into paths of length roughly $d$, giving an approximate solution to a problem of Kotzig from 1957. Along the way, we show that almost all of the vertices of a $d$-regular graph can be partitioned into $n/(d&#43;1)$ paths, asymptotically confirming a conjecture of Magnant and Martin from 2009.</td>
                <td>Submitted 4 June, 2024; </td>
                <td>34 pages, 1 figure</td>
            </tr>
        
            <tr class="item">
                
                <td><a href="https://arxiv.org/abs/2406.02427">arXiv:2406.02427</a></td>
                <td>Bipartite entanglement of noisy stabilizer states through the lens of stabilizer codes</td>
                <td>Kenneth Goodenough, Aqil Sajjad, Eneet Kaur, Saikat Guha, Don Towsley</td>
                <td>Stabilizer states are a prime resource for a number of applications in quantum information science, such as secret-sharing and measurement-based quantum computation. This motivates us to study the entanglement of noisy stabilizer states across a bipartition. We show that the spectra of the corresponding reduced states can be expressed in terms of properties of an associated stabilizer code. In particular, this allows us to show that the coherent information is related to the so-called syndrome entropy of the underlying code. We use this viewpoint to find stabilizer states that are resilient against noise, allowing for more robust entanglement distribution in near-term quantum networks. We specialize our results to the case of graph states, where the found connections with stabilizer codes reduces back to classical linear codes for dephasing noise. On our way we provide an alternative proof of the fact that every qubit stabilizer code is equivalent up to single-qubit Clifford gates to a graph code.</td>
                <td>Submitted 4 June, 2024; </td>
                <td>7 pages, 4 figures</td>
            </tr>
        
            <tr class="item">
                
                <td><a href="https://arxiv.org/abs/2406.02397">arXiv:2406.02397</a></td>
                <td>One-arm Probabilities for Metric Graph Gaussian Free Fields below and at the Critical Dimension</td>
                <td>Zhenhao Cai, Jian Ding</td>
                <td>For the critical level-set of the Gaussian free field on the metric graph of $\mathbb Z^d$, we consider the one-arm probability $θ_d(N)$, i.e., the probability that the boundary of a box of side length $2N$ is connected to the center. We prove that $θ_d(N)$ is $O(N^{-\frac{d}{2}&#43;1})$ for $3\le d\le 5$, and is $N^{-2&#43;o(1)}$ for $d=6$. Our upper bounds match the lower bounds in a previous work by Ding and Wirth up to a constant factor for $3\le d\le 5$, and match the exponent therein for $d=6$. Combined with our previous result that $θ_d(N) \asymp N^{-2}$ for $d&gt;6$, this seems to present the first percolation model whose one-arm probabilities are essentially completely understood in all dimensions. In particular, these results fully confirm Werner&#39;s conjectures (2021) on the one-arm exponents:</td>
                <td>Submitted 4 June, 2024; </td>
                <td></td>
            </tr>
        
            <tr class="item">
                
                <td><a href="https://arxiv.org/abs/2406.02395">arXiv:2406.02395</a></td>
                <td>GrootVL: Tree Topology is All You Need in State Space Model</td>
                <td>Yicheng Xiao, Lin Song, Shaoli Huang, Jiangshan Wang, Siyu Song, Yixiao Ge, Xiu Li, Ying Shan</td>
                <td>The state space models, employing recursively propagated features, demonstrate strong representation capabilities comparable to Transformer models and superior efficiency. However, constrained by the inherent geometric constraints of sequences, it still falls short in modeling long-range dependencies. To address this issue, we propose the GrootVL network, which first dynamically generates a tree topology based on spatial relationships and input features. Then, feature propagation is performed based on this graph, thereby breaking the original sequence constraints to achieve stronger representation capabilities. Additionally, we introduce a linear complexity dynamic programming algorithm to enhance long-range interactions without increasing computational cost. GrootVL is a versatile multimodal framework that can be applied to both visual and textual tasks. Extensive experiments demonstrate that our method significantly outperforms existing structured state space models on image classification, object detection and segmentation. Besides, by fine-tuning large language models, our approach achieves consistent improvements in multiple textual tasks at minor training cost.</td>
                <td>Submitted 4 June, 2024; </td>
                <td>The code is available at https://github.com/EasonXiao-888/GrootVL</td>
            </tr>
        
            <tr class="item">
                
                <td><a href="https://arxiv.org/abs/2406.02381">arXiv:2406.02381</a></td>
                <td>Kirigami: large convolutional kernels improve deep learning-based RNA secondary structure prediction</td>
                <td>Marc Harary, Chengxin Zhang, Anna Marie Pyle</td>
                <td>We introduce a novel fully convolutional neural network (FCN) architecture for predicting the secondary structure of ribonucleic acid (RNA) molecules. Interpreting RNA structures as weighted graphs, we employ deep learning to estimate the probability of base pairing between nucleotide residues. Unique to our model are its massive 11-pixel kernels, which we argue provide a distinct advantage for FCNs on the specialized domain of RNA secondary structures. On a widely adopted, standardized test set comprised of 1,305 molecules, the accuracy of our method exceeds that of current state-of-the-art (SOTA) secondary structure prediction software, achieving a Matthews Correlation Coefficient (MCC) over 11-40% higher than that of other leading methods on overall structures and 58-400% higher on pseudoknots specifically.</td>
                <td>Submitted 4 June, 2024; </td>
                <td></td>
            </tr>
        
            <tr class="item">
                
                <td><a href="https://arxiv.org/abs/2406.02377">arXiv:2406.02377</a></td>
                <td>XRec: Large Language Models for Explainable Recommendation</td>
                <td>Qiyao Ma, Xubin Ren, Chao Huang</td>
                <td>Recommender systems help users navigate information overload by providing personalized recommendations aligned with their preferences. Collaborative Filtering (CF) is a widely adopted approach, but while advanced techniques like graph neural networks (GNNs) and self-supervised learning (SSL) have enhanced CF models for better user representations, they often lack the ability to provide explanations for the recommended items. Explainable recommendations aim to address this gap by offering transparency and insights into the recommendation decision-making process, enhancing users&#39; understanding. This work leverages the language capabilities of Large Language Models (LLMs) to push the boundaries of explainable recommender systems. We introduce a model-agnostic framework called XRec, which enables LLMs to provide comprehensive explanations for user behaviors in recommender systems. By integrating collaborative signals and designing a lightweight collaborative adaptor, the framework empowers LLMs to understand complex patterns in user-item interactions and gain a deeper understanding of user preferences. Our extensive experiments demonstrate the effectiveness of XRec, showcasing its ability to generate comprehensive and meaningful explanations that outperform baseline approaches in explainable recommender systems. We open-source our model implementation at https://github.com/HKUDS/XRec.</td>
                <td>Submitted 4 June, 2024; </td>
                <td></td>
            </tr>
        
            <tr class="item">
                
                <td><a href="https://arxiv.org/abs/2406.02362">arXiv:2406.02362</a></td>
                <td>Temporal Graph Rewiring with Expander Graphs</td>
                <td>Katarina Petrović, Shenyang Huang, Farimah Poursafaei, Petar Veličković</td>
                <td>Evolving relations in real-world networks are often modelled by temporal graphs. Graph rewiring techniques have been utilised on Graph Neural Networks (GNNs) to improve expressiveness and increase model performance. In this work, we propose Temporal Graph Rewiring (TGR), the first approach for graph rewiring on temporal graphs. TGR enables communication between temporally distant nodes in a continuous time dynamic graph by utilising expander graph propagation to construct a message passing highway for message passing between distant nodes. Expander graphs are suitable candidates for rewiring as they help overcome the oversquashing problem often observed in GNNs. On the public tgbl-wiki benchmark, we show that TGR improves the performance of a widely used TGN model by a significant margin. Our code repository is accessible at https://anonymous.4open.science/r/TGR-254C.</td>
                <td>Submitted 4 June, 2024; </td>
                <td>10 pages, 2 figures</td>
            </tr>
        
            <tr class="item">
                
                <td><a href="https://arxiv.org/abs/2406.02348">arXiv:2406.02348</a></td>
                <td>AMOSL: Adaptive Modality-wise Structure Learning in Multi-view Graph Neural Networks For Enhanced Unified Representation</td>
                <td>Peiyu Liang, Hongchang Gao, Xubin He</td>
                <td>While Multi-view Graph Neural Networks (MVGNNs) excel at leveraging diverse modalities for learning object representation, existing methods assume identical local topology structures across modalities that overlook real-world discrepancies. This leads MVGNNs straggles in modality fusion and representations denoising. To address these issues, we propose adaptive modality-wise structure learning (AMoSL). AMoSL captures node correspondences between modalities via optimal transport, and jointly learning with graph embedding. To enable efficient end-to-end training, we employ an efficient solution for the resulting complex bilevel optimization problem. Furthermore, AMoSL adapts to downstream tasks through unsupervised learning on inter-modality distances. The effectiveness of AMoSL is demonstrated by its ability to train more accurate graph classifiers on six benchmark datasets.</td>
                <td>Submitted 4 June, 2024; </td>
                <td></td>
            </tr>
        
            <tr class="item">
                
                <td><a href="https://arxiv.org/abs/2406.02343">arXiv:2406.02343</a></td>
                <td>Cluster-Aware Similarity Diffusion for Instance Retrieval</td>
                <td>Jifei Luo, Hantao Yao, Changsheng Xu</td>
                <td>Diffusion-based re-ranking is a common method used for retrieving instances by performing similarity propagation in a nearest neighbor graph. However, existing techniques that construct the affinity graph based on pairwise instances can lead to the propagation of misinformation from outliers and other manifolds, resulting in inaccurate results. To overcome this issue, we propose a novel Cluster-Aware Similarity (CAS) diffusion for instance retrieval. The primary concept of CAS is to conduct similarity diffusion within local clusters, which can reduce the influence from other manifolds explicitly. To obtain a symmetrical and smooth similarity matrix, our Bidirectional Similarity Diffusion strategy introduces an inverse constraint term to the optimization objective of local cluster diffusion. Additionally, we have optimized a Neighbor-guided Similarity Smoothing approach to ensure similarity consistency among the local neighbors of each instance. Evaluations in instance retrieval and object re-identification validate the effectiveness of the proposed CAS, our code is publicly available.</td>
                <td>Submitted 4 June, 2024; </td>
                <td></td>
            </tr>
        
            <tr class="item">
                
                <td><a href="https://arxiv.org/abs/2406.02299">arXiv:2406.02299</a></td>
                <td>On the structure of Kauffman bracket skein algebra of a surface</td>
                <td>Haimiao Chen</td>
                <td>Suppose $R$ is a commutative ring with identity and a fixed invertible element $q^{\frac{1}{2}}$ such that $q&#43;q^{-1}$ is invertible.</td>
                <td>Submitted 4 June, 2024; </td>
                <td>20 pages, 18 figures</td>
            </tr>
        
            <tr class="item">
                
                <td><a href="https://arxiv.org/abs/2406.02283">arXiv:2406.02283</a></td>
                <td>Broadcasting Support Relations Recursively from Local Dynamics for Object Retrieval in Clutters</td>
                <td>Yitong Li, Ruihai Wu, Haoran Lu, Chuanruo Ning, Yan Shen, Guanqi Zhan, Hao Dong</td>
                <td>In our daily life, cluttered objects are everywhere, from scattered stationery and books cluttering the table to bowls and plates filling the kitchen sink. Retrieving a target object from clutters is an essential while challenging skill for robots, for the difficulty of safely manipulating an object without disturbing others, which requires the robot to plan a manipulation sequence and first move away a few other objects supported by the target object step by step. However, due to the diversity of object configurations (e.g., categories, geometries, locations and poses) and their combinations in clutters, it is difficult for a robot to accurately infer the support relations between objects faraway with various objects in between. In this paper, we study retrieving objects in complicated clutters via a novel method of recursively broadcasting the accurate local dynamics to build a support relation graph of the whole scene, which largely reduces the complexity of the support relation inference and improves the accuracy. Experiments in both simulation and the real world demonstrate the efficiency and effectiveness of our method.</td>
                <td>Submitted 4 June, 2024; </td>
                <td>RSS 2024</td>
            </tr>
        
            <tr class="item">
                
                <td><a href="https://arxiv.org/abs/2406.02269">arXiv:2406.02269</a></td>
                <td>Graph Neural Networks Do Not Always Oversmooth</td>
                <td>Bastian Epping, Alexandre René, Moritz Helias, Michael T. Schaub</td>
                <td>Graph neural networks (GNNs) have emerged as powerful tools for processing relational data in applications. However, GNNs suffer from the problem of oversmoothing, the property that the features of all nodes exponentially converge to the same vector over layers, prohibiting the design of deep GNNs. In this work we study oversmoothing in graph convolutional networks (GCNs) by using their Gaussian process (GP) equivalence in the limit of infinitely many hidden features. By generalizing methods from conventional deep neural networks (DNNs), we can describe the distribution of features at the output layer of deep GCNs in terms of a GP: as expected, we find that typical parameter choices from the literature lead to oversmoothing. The theory, however, allows us to identify a new, nonoversmoothing phase: if the initial weights of the network have sufficiently large variance, GCNs do not oversmooth, and node features remain informative even at large depth. We demonstrate the validity of this prediction in finite-size GCNs by training a linear classifier on their output. Moreover, using the linearization of the GCN GP, we generalize the concept of propagation depth of information from DNNs to GCNs. This propagation depth diverges at the transition between the oversmoothing and non-oversmoothing phase. We test the predictions of our approach and find good agreement with finite-size GCNs. Initializing GCNs near the transition to the non-oversmoothing phase, we obtain networks which are both deep and expressive.</td>
                <td>Submitted 4 June, 2024; </td>
                <td></td>
            </tr>
        
            <tr class="item">
                
                <td><a href="https://arxiv.org/abs/2406.02259">arXiv:2406.02259</a></td>
                <td>Restricted SDC Edge Cover Pebbling Number</td>
                <td>A. Lourdusamy, F. Joy Beaula, F. Patrick, I. Dhivviyanandam</td>
                <td>The restricted edge pebbling distribution is a distribution of pebbles on the edges of $G$ is the placement of pebbles on the edges with the restriction that only an even number of pebbles should be placed on the edges with labels $0$. Given an SDC labeling of $G$, the restricted SDC edge cover pebbling number of a graph $G$, $ψ_{EC}(G)$, is the least positive integer $m$ for which any restricted edge pebbling distribution of $m$ pebbles such that at the end there are no pebbles on the edges having label $0$ will allow the shifting of a pebble simultaneously to all edges with label $1$ using a sequence of restricted edge pebbling moves. We compute the restricted SDC edge cover pebbling number for some graphs</td>
                <td>Submitted 4 June, 2024; </td>
                <td>9</td>
            </tr>
        
            <tr class="item">
                
                <td><a href="https://arxiv.org/abs/2406.02244">arXiv:2406.02244</a></td>
                <td>On the characterization of chordal graphs using Horn hypergeometric series</td>
                <td>Dipnit Biswas, Irfan Habib, R. Venkatesh</td>
                <td>In 6, Radchenko and Villegas characterized the chordal graphs by their inverse of the independence polynomials being Horn hypergeometric series. In this paper, we reprove their result using some elementary combinatorial methods and also generalize it to PEO graphs that could have a countable number of vertices. Our proof is different from the proof of 6, and it is based on the connection between the inverse of the multi-variate independence polynomials and the multi-colored chromatic polynomials of graphs, established in 1.</td>
                <td>Submitted 4 June, 2024; </td>
                <td>11 pages</td>
            </tr>
        
            <tr class="item">
                
                <td><a href="https://arxiv.org/abs/2406.02205">arXiv:2406.02205</a></td>
                <td>Query-Enhanced Adaptive Semantic Path Reasoning for Inductive Knowledge Graph Completion</td>
                <td>Kai Sun, Jiapu Wang, Huajie Jiang, Yongli Hu, Baocai Yin</td>
                <td>Conventional Knowledge graph completion (KGC) methods aim to infer missing information in incomplete Knowledge Graphs (KGs) by leveraging existing information, which struggle to perform effectively in scenarios involving emerging entities. Inductive KGC methods can handle the emerging entities and relations in KGs, offering greater dynamic adaptability. While existing inductive KGC methods have achieved some success, they also face challenges, such as susceptibility to noisy structural information during reasoning and difficulty in capturing long-range dependencies in reasoning paths. To address these challenges, this paper proposes the Query-Enhanced Adaptive Semantic Path Reasoning (QASPR) framework, which simultaneously captures both the structural and semantic information of KGs to enhance the inductive KGC task. Specifically, the proposed QASPR employs a query-dependent masking module to adaptively mask noisy structural information while retaining important information closely related to the targets. Additionally, QASPR introduces a global semantic scoring module that evaluates both the individual contributions and the collective impact of nodes along the reasoning path within KGs. The experimental results demonstrate that QASPR achieves state-of-the-art performance.</td>
                <td>Submitted 4 June, 2024; </td>
                <td></td>
            </tr>
        
            <tr class="item">
                
                <td><a href="https://arxiv.org/abs/2406.02199">arXiv:2406.02199</a></td>
                <td>Communication Complexity of Graph Isomorphism, Coloring, and Distance Games</td>
                <td>Pierre Botteron, Moritz Weber</td>
                <td>In quantum information, nonlocal games are particularly useful for differentiating classical, quantum, and non-signalling correlations. An example of differentiation is given by the principle of no-collapse of communication complexity, which is often interpreted as necessary for a feasible physical theory. It is satisfied by quantum correlations but violated by some non-signalling ones.</td>
                <td>Submitted 4 June, 2024; </td>
                <td>42 pages, 9 figures</td>
            </tr>
        
            <tr class="item">
                
                <td><a href="https://arxiv.org/abs/2406.02189">arXiv:2406.02189</a></td>
                <td>Fast and Scalable Multi-Kernel Encoder Classifier</td>
                <td>Cencheng Shen</td>
                <td>This paper introduces a new kernel-based classifier by viewing kernel matrices as generalized graphs and leveraging recent progress in graph embedding techniques. The proposed method facilitates fast and scalable kernel matrix embedding, and seamlessly integrates multiple kernels to enhance the learning process. Our theoretical analysis offers a population-level characterization of this approach using random variables. Empirically, our method demonstrates superior running time compared to standard approaches such as support vector machines and two-layer neural network, while achieving comparable classification accuracy across various simulated and real datasets.</td>
                <td>Submitted 4 June, 2024; </td>
                <td>12 pages main &#43; 3 pages appendix</td>
            </tr>
        
            <tr class="item">
                
                <td><a href="https://arxiv.org/abs/2406.02187">arXiv:2406.02187</a></td>
                <td>DNCs Require More Planning Steps</td>
                <td>Yara Shamshoum, Nitzan Hodos, Yuval Sieradzki, Assaf Schuster</td>
                <td>Many recent works use machine learning models to solve various complex algorithmic problems. However, these models attempt to reach a solution without considering the problem&#39;s required computational complexity, which can be detrimental to their ability to solve it correctly. In this work we investigate the effect of computational time and memory on generalization of implicit algorithmic solvers. To do so, we focus on the Differentiable Neural Computer (DNC), a general problem solver that also lets us reason directly about its usage of time and memory. In this work, we argue that the number of planning steps the model is allowed to take, which we call &#34;planning budget&#34;, is a constraint that can cause the model to generalize poorly and hurt its ability to fully utilize its external memory. We evaluate our method on Graph Shortest Path, Convex Hull, Graph MinCut and Associative Recall, and show how the planning budget can drastically change the behavior of the learned algorithm, in terms of learned time complexity, training time, stability and generalization to inputs larger than those seen during training.</td>
                <td>Submitted 4 June, 2024; </td>
                <td></td>
            </tr>
        
            <tr class="item">
                
                <td><a href="https://arxiv.org/abs/2406.02184">arXiv:2406.02184</a></td>
                <td>GraVITON: Graph based garment warping with attention guided inversion for Virtual-tryon</td>
                <td>Sanhita Pathak, Vinay Kaushik, Brejesh Lall</td>
                <td>Virtual try-on, a rapidly evolving field in computer vision, is transforming e-commerce by improving customer experiences through precise garment warping and seamless integration onto the human body. While existing methods such as TPS and flow address the garment warping but overlook the finer contextual details. In this paper, we introduce a novel graph based warping technique which emphasizes the value of context in garment flow. Our graph based warping module generates warped garment as well as a coarse person image, which is utilised by a simple refinement network to give a coarse virtual tryon image. The proposed work exploits latent diffusion model to generate the final tryon, treating garment transfer as an inpainting task. The diffusion model is conditioned with decoupled cross attention based inversion of visual and textual information. We introduce an occlusion aware warping constraint that generates dense warped garment, without any holes and occlusion. Our method, validated on VITON-HD and Dresscode datasets, showcases substantial state-of-the-art qualitative and quantitative results showing considerable improvement in garment warping, texture preservation, and overall realism.</td>
                <td>Submitted 4 June, 2024; </td>
                <td>18 pages, 7 Figures and 6 Tables</td>
            </tr>
        
            <tr class="item">
                
                <td><a href="https://arxiv.org/abs/2406.02156">arXiv:2406.02156</a></td>
                <td>Almost linear time differentially private release of synthetic graphs</td>
                <td>Jingcheng Liu, Jalaj Upadhyay, Zongrui Zou</td>
                <td>In this paper, we give an almost linear time and space algorithms to sample from an exponential mechanism with an $\ell_1$-score function defined over an exponentially large non-convex set. As a direct result, on input an $n$ vertex $m$ edges graph $G$, we present the \textit{first} $\widetilde{O}(m)$ time and $O(m)$ space algorithms for differentially privately outputting an $n$ vertex $O(m)$ edges synthetic graph that approximates all the cuts and the spectrum of $G$. These are the \emph{first} private algorithms for releasing synthetic graphs that nearly match this task&#39;s time and space complexity in the non-private setting while achieving the same (or better) utility as the previous works in the more practical sparse regime. Additionally, our algorithms can be extended to private graph analysis under continual observation.</td>
                <td>Submitted 4 June, 2024; </td>
                <td></td>
            </tr>
        
            <tr class="item">
                
                <td><a href="https://arxiv.org/abs/2406.02110">arXiv:2406.02110</a></td>
                <td>UniOQA: A Unified Framework for Knowledge Graph Question Answering with Large Language Models</td>
                <td>Zhuoyang Li, Liran Deng, Hui Liu, Qiaoqiao Liu, Junzhao Du</td>
                <td>OwnThink stands as the most extensive Chinese open-domain knowledge graph introduced in recent times. Despite prior attempts in question answering over OwnThink (OQA), existing studies have faced limitations in model representation capabilities, posing challenges in further enhancing overall accuracy in question answering. In this paper, we introduce UniOQA, a unified framework that integrates two complementary parallel workflows. Unlike conventional approaches, UniOQA harnesses large language models (LLMs) for precise question answering and incorporates a direct-answer-prediction process as a cost-effective complement. Initially, to bolster representation capacity, we fine-tune an LLM to translate questions into the Cypher query language (CQL), tackling issues associated with restricted semantic understanding and hallucinations. Subsequently, we introduce the Entity and Relation Replacement algorithm to ensure the executability of the generated CQL. Concurrently, to augment overall accuracy in question answering, we further adapt the Retrieval-Augmented Generation (RAG) process to the knowledge graph. Ultimately, we optimize answer accuracy through a dynamic decision algorithm. Experimental findings illustrate that UniOQA notably advances SpCQL Logical Accuracy to 21.2% and Execution Accuracy to 54.9%, achieving the new state-of-the-art results on this benchmark. Through ablation experiments, we delve into the superior representation capacity of UniOQA and quantify its performance breakthrough.</td>
                <td>Submitted 4 June, 2024; </td>
                <td>10 pages, 5 figures</td>
            </tr>
        
            <tr class="item">
                
                <td><a href="https://arxiv.org/abs/2406.02107">arXiv:2406.02107</a></td>
                <td>Degrees are Useless in SNORT When Measuring Temperature</td>
                <td>Svenja Huntemann, Tomasz Maciosowski</td>
                <td>Snort is a two-player game played on a simple graph in which players alternately colour a vertex such that they do not colour adjacent to their opponents&#39; vertex. In combinatorial game theory, the temperature of a position is a measure of the urgency of moving first. It is known that the temperature of \snort in general is infinite ($K_{1,n}$ has temperature $n$). We show that the temperature in addition can be infinitely larger than the degree of the board being played on. We do so by constructing a family of positions in which the temperature grows twice as fast as the degree of the board.</td>
                <td>Submitted 4 June, 2024; </td>
                <td></td>
            </tr>
        
            <tr class="item">
                
                <td><a href="https://arxiv.org/abs/2406.02096">arXiv:2406.02096</a></td>
                <td>MS-Mapping: Multi-session LiDAR Mapping with Wasserstein-based Keyframe Selection</td>
                <td>Xiangcheng Hu, Jin Wu, Jianhao Jiao, Wei Zhang, Ping Tan</td>
                <td>Large-scale multi-session LiDAR mapping plays a crucial role in various applications but faces significant challenges in data redundancy and pose graph scalability. This paper present MS-Mapping, a novel multi-session LiDAR mapping system that combines an incremental mapping scheme with support for various LiDAR-based odometry, enabling high-precision and consistent map assembly in large-scale environments. Our approach introduces a real-time keyframe selection method based on the Wasserstein distance, which effectively reduces data redundancy and pose graph complexity. We formulate the LiDAR point cloud keyframe selection problem using a similarity method based on Gaussian mixture models (GMM) and tackle the real-time challenge by employing an incremental voxel update method. Extensive experiments on large-scale campus scenes and over \SI{12.8}{km} of public and self-collected datasets demonstrate the efficiency, accuracy, and consistency of our map assembly approach. To facilitate further research and development in the community, we make our code https://github.com/JokerJohn/MS-Mapping and datasets publicly available.</td>
                <td>Submitted 4 June, 2024; </td>
                <td>5 pages, 4 figures</td>
            </tr>
        
            <tr class="item">
                
                <td><a href="https://arxiv.org/abs/2406.02059">arXiv:2406.02059</a></td>
                <td>Graph Adversarial Diffusion Convolution</td>
                <td>Songtao Liu, Jinghui Chen, Tianfan Fu, Lu Lin, Marinka Zitnik, Dinghao Wu</td>
                <td>This paper introduces a min-max optimization formulation for the Graph Signal Denoising (GSD) problem. In this formulation, we first maximize the second term of GSD by introducing perturbations to the graph structure based on Laplacian distance and then minimize the overall loss of the GSD. By solving the min-max optimization problem, we derive a new variant of the Graph Diffusion Convolution (GDC) architecture, called Graph Adversarial Diffusion Convolution (GADC). GADC differs from GDC by incorporating an additional term that enhances robustness against adversarial attacks on the graph structure and noise in node features. Moreover, GADC improves the performance of GDC on heterophilic graphs. Extensive experiments demonstrate the effectiveness of GADC across various datasets. Code is available at https://github.com/SongtaoLiu0823/GADC.</td>
                <td>Submitted 4 June, 2024; </td>
                <td>Accepted by ICML 2024</td>
            </tr>
        
            <tr class="item">
                
                <td><a href="https://arxiv.org/abs/2406.02056">arXiv:2406.02056</a></td>
                <td>CAP: A Context-Aware Neural Predictor for NAS</td>
                <td>Han Ji, Yuqi Feng, Yanan Sun</td>
                <td>Neural predictors are effective in boosting the time-consuming performance evaluation stage in neural architecture search (NAS), owing to their direct estimation of unseen architectures. Despite the effectiveness, training a powerful neural predictor with fewer annotated architectures remains a huge challenge. In this paper, we propose a context-aware neural predictor (CAP) which only needs a few annotated architectures for training based on the contextual information from the architectures. Specifically, the input architectures are encoded into graphs and the predictor infers the contextual structure around the nodes inside each graph. Then, enhanced by the proposed context-aware self-supervised task, the pre-trained predictor can obtain expressive and generalizable representations of architectures. Therefore, only a few annotated architectures are sufficient for training. Experimental results in different search spaces demonstrate the superior performance of CAP compared with state-of-the-art neural predictors. In particular, CAP can rank architectures precisely at the budget of only 172 annotated architectures in NAS-Bench-101. Moreover, CAP can help find promising architectures in both NAS-Bench-101 and DARTS search spaces on the CIFAR-10 dataset, serving as a useful navigator for NAS to explore the search space efficiently.</td>
                <td>Submitted 4 June, 2024; </td>
                <td>Accepted by IJCAI24</td>
            </tr>
        
            <tr class="item">
                
                <td><a href="https://arxiv.org/abs/2406.02049">arXiv:2406.02049</a></td>
                <td>Causal Effect Identification in LiNGAM Models with Latent Confounders</td>
                <td>Daniele Tramontano, Yaroslav Kivva, Saber Salehkaleybar, Mathias Drton, Negar Kiyavash</td>
                <td>We study the generic identifiability of causal effects in linear non-Gaussian acyclic models (LiNGAM) with latent variables. We consider the problem in two main settings: When the causal graph is known a priori, and when it is unknown. In both settings, we provide a complete graphical characterization of the identifiable direct or total causal effects among observed variables. Moreover, we propose efficient algorithms to certify the graphical conditions. Finally, we propose an adaptation of the reconstruction independent component analysis (RICA) algorithm that estimates the causal effects from the observational data given the causal graph. Experimental results show the effectiveness of the proposed method in estimating the causal effects.</td>
                <td>Submitted 4 June, 2024; </td>
                <td>Accepted at International Conference on Machine Learning (ICML) 2024</td>
            </tr>
        
            <tr class="item">
                
                <td><a href="https://arxiv.org/abs/2406.02040">arXiv:2406.02040</a></td>
                <td>DFA-GNN: Forward Learning of Graph Neural Networks by Direct Feedback Alignment</td>
                <td>Gongpei Zhao, Tao Wang, Congyan Lang, Yi Jin, Yidong Li, Haibin Ling</td>
                <td>Graph neural networks are recognized for their strong performance across various applications, with the backpropagation algorithm playing a central role in the development of most GNN models. However, despite its effectiveness, BP has limitations that challenge its biological plausibility and affect the efficiency, scalability and parallelism of training neural networks for graph-based tasks. While several non-BP training algorithms, such as the direct feedback alignment, have been successfully applied to fully-connected and convolutional network components for handling Euclidean data, directly adapting these non-BP frameworks to manage non-Euclidean graph data in GNN models presents significant challenges. These challenges primarily arise from the violation of the i.i.d. assumption in graph data and the difficulty in accessing prediction errors for all samples (nodes) within the graph. To overcome these obstacles, in this paper we propose DFA-GNN, a novel forward learning framework tailored for GNNs with a case study of semi-supervised learning. The proposed method breaks the limitations of BP by using a dedicated forward training mechanism. Specifically, DFA-GNN extends the principles of DFA to adapt to graph data and unique architecture of GNNs, which incorporates the information of graph topology into the feedback links to accommodate the non-Euclidean characteristics of graph data. Additionally, for semi-supervised graph learning tasks, we developed a pseudo error generator that spreads residual errors from training data to create a pseudo error for each unlabeled node. These pseudo errors are then utilized to train GNNs using DFA. Extensive experiments on 10 public benchmarks reveal that our learning framework outperforms not only previous non-BP methods but also the standard BP methods, and it exhibits excellent robustness against various types of noise and attacks.</td>
                <td>Submitted 4 June, 2024; </td>
                <td></td>
            </tr>
        
            <tr class="item">
                
                <td><a href="https://arxiv.org/abs/2406.02038">arXiv:2406.02038</a></td>
                <td>Leveraging Predicate and Triplet Learning for Scene Graph Generation</td>
                <td>Jiankai Li, Yunhong Wang, Xiefan Guo, Ruijie Yang, Weixin Li</td>
                <td>Scene Graph Generation (SGG) aims to identify entities and predict the relationship triplets \textit{\textless subject, predicate, object\textgreater } in visual scenes. Given the prevalence of large visual variations of subject-object pairs even in the same predicate, it can be quite challenging to model and refine predicate representations directly across such pairs, which is however a common strategy adopted by most existing SGG methods. We observe that visual variations within the identical triplet are relatively small and certain relation cues are shared in the same type of triplet, which can potentially facilitate the relation learning in SGG. Moreover, for the long-tail problem widely studied in SGG task, it is also crucial to deal with the limited types and quantity of triplets in tail predicates. Accordingly, in this paper, we propose a Dual-granularity Relation Modeling (DRM) network to leverage fine-grained triplet cues besides the coarse-grained predicate ones. DRM utilizes contexts and semantics of predicate and triplet with Dual-granularity Constraints, generating compact and balanced representations from two perspectives to facilitate relation recognition. Furthermore, a Dual-granularity Knowledge Transfer (DKT) strategy is introduced to transfer variation from head predicates/triplets to tail ones, aiming to enrich the pattern diversity of tail classes to alleviate the long-tail problem. Extensive experiments demonstrate the effectiveness of our method, which establishes new state-of-the-art performance on Visual Genome, Open Image, and GQA datasets. Our code is available at \url{https://github.com/jkli1998/DRM}</td>
                <td>Submitted 4 June, 2024; </td>
                <td>CVPR 2024</td>
            </tr>
        
            <tr class="item">
                
                <td><a href="https://arxiv.org/abs/2406.02030">arXiv:2406.02030</a></td>
                <td>Multimodal Reasoning with Multimodal Knowledge Graph</td>
                <td>Junlin Lee, Yequan Wang, Jing Li, Min Zhang</td>
                <td>Multimodal reasoning with large language models (LLMs) often suffers from hallucinations and the presence of deficient or outdated knowledge within LLMs. Some approaches have sought to mitigate these issues by employing textual knowledge graphs, but their singular modality of knowledge limits comprehensive cross-modal understanding. In this paper, we propose the Multimodal Reasoning with Multimodal Knowledge Graph (MR-MKG) method, which leverages multimodal knowledge graphs (MMKGs) to learn rich and semantic knowledge across modalities, significantly enhancing the multimodal reasoning capabilities of LLMs. In particular, a relation graph attention network is utilized for encoding MMKGs and a cross-modal alignment module is designed for optimizing image-text alignment. A MMKG-grounded dataset is constructed to equip LLMs with initial expertise in multimodal reasoning through pretraining. Remarkably, MR-MKG achieves superior performance while training on only a small fraction of parameters, approximately 2.25% of the LLM&#39;s parameter size. Experimental results on multimodal question answering and multimodal analogy reasoning tasks demonstrate that our MR-MKG method outperforms previous state-of-the-art models.</td>
                <td>Submitted 4 June, 2024; </td>
                <td></td>
            </tr>
        
            <tr class="item">
                
                <td><a href="https://arxiv.org/abs/2406.02014">arXiv:2406.02014</a></td>
                <td>Understanding Auditory Evoked Brain Signal via Physics-informed Embedding Network with Multi-Task Transformer</td>
                <td>Wanli Ma, Xuegang Tang, Jin Gu, Ying Wang, Yuling Xia</td>
                <td>In the fields of brain-computer interaction and cognitive neuroscience, effective decoding of auditory signals from task-based functional magnetic resonance imaging (fMRI) is key to understanding how the brain processes complex auditory information. Although existing methods have enhanced decoding capabilities, limitations remain in information utilization and model representation. To overcome these challenges, we propose an innovative multi-task learning model, Physics-informed Embedding Network with Multi-Task Transformer (PEMT-Net), which enhances decoding performance through physics-informed embedding and deep learning techniques. PEMT-Net consists of two principal components: feature augmentation and classification. For feature augmentation, we propose a novel approach by creating neural embedding graphs via node embedding, utilizing random walks to simulate the physical diffusion of neural information. This method captures both local and non-local information overflow and proposes a position encoding based on relative physical coordinates. In the classification segment, we propose adaptive embedding fusion to maximally capture linear and non-linear characteristics. Furthermore, we propose an innovative parameter-sharing mechanism to optimize the retention and learning of extracted features. Experiments on a specific dataset demonstrate PEMT-Net&#39;s significant performance in multi-task auditory signal decoding, surpassing existing methods and offering new insights into the brain&#39;s mechanisms for processing complex auditory information.</td>
                <td>Submitted 4 June, 2024; </td>
                <td></td>
            </tr>
        
            <tr class="item">
                
                <td><a href="https://arxiv.org/abs/2406.02012">arXiv:2406.02012</a></td>
                <td>Improved Generalized Automorphism Belief Propagation Decoding</td>
                <td>Jonathan Mandelbaum, Sisi Miao, Nils Albert Schwendemann, Holger Jäkel, Laurent Schmalen</td>
                <td>With the increasing demands on future wireless systems, new design objectives become eminent. Low-density parity-check codes together with belief propagation (BP) decoding have outstanding performance for large block lengths. Yet, for future wireless systems, good decoding performance for short block lengths is mandatory, a regime in which BP decoding typically shows a significant gap to maximum likelihood decoding. Automorphism ensemble decoding (AED) is known to reduce this gap effectively and, in addition, enables an easy trade-off between latency, throughput, and complexity. Recently, generalized AED (GAED) was proposed to increase the set of feasible automorphisms suitable for ensemble decoding. By construction, GAED requires a preprocessing step within its constituent paths that results in information loss and potentially limits the gains of GAED. In this work, we show that the preprocessing step can be merged with the Tanner graph of BP decoding, thereby improving the performance of the constituent paths. Finally, we show that the improvement of the individual paths also enhances the overall performance of the ensemble.</td>
                <td>Submitted 4 June, 2024; </td>
                <td>Accepted for presentation at ISWCS&#39;24</td>
            </tr>
        
            <tr class="item">
                
                <td><a href="https://arxiv.org/abs/2406.01999">arXiv:2406.01999</a></td>
                <td>Random Abstract Cell Complexes</td>
                <td>Josef Hoppe, Michael T. Schaub</td>
                <td>We define a model for random (abstract) cell complexes (CCs), similiar to the well-known Erdős-Rényi model for graphs and its extensions for simplicial complexes. To build a random cell complex, we first draw from an Erdős-Rényi graph, and consecutively augment the graph with cells for each dimension with a specified probability. As the number of possible cells increases combinatorially -- e.g., 2-cells can be represented as cycles, or permutations -- we derive an approximate sampling algorithm for this model limited to two-dimensional abstract cell complexes. Since there is a large variance in the number of simple cycles on graphs drawn from the same configuration of ER, we also provide an efficient method to approximate that number, which is of independent interest. Moreover, it enables us to specify the expected number of 2-cells of each boundary length we want to sample. We provide some initial analysis into the properties of random CCs drawn from this model. We further showcase practical applications for our random CCs as null models, and in the context of (random) liftings of graphs to cell complexes. Both the sampling and cycle count estimation algorithms are available in the package `py-raccoon` on the Python Packaging Index.</td>
                <td>Submitted 4 June, 2024; </td>
                <td>10 pages, 8 figures (plus appendix). For evaluation code, see https://github.com/josefhoppe/random-abstract-cell-complexes</td>
            </tr>
        
            <tr class="item">
                
                <td><a href="https://arxiv.org/abs/2406.01996">arXiv:2406.01996</a></td>
                <td>Bayesian Mesh Optimization for Graph Neural Networks to Enhance Engineering Performance Prediction</td>
                <td>Jangseop Park, Namwoo Kang</td>
                <td>In engineering design, surrogate models are widely employed to replace computationally expensive simulations by leveraging design variables and geometric parameters from computer-aided design (CAD) models. However, these models often lose critical information when simplified to lower dimensions and face challenges in parameter definition, especially with the complex 3D shapes commonly found in industrial datasets. To address these limitations, we propose a Bayesian graph neural network (GNN) framework for a 3D deep-learning-based surrogate model that predicts engineering performance by directly learning geometric features from CAD using mesh representation. Our framework determines the optimal size of mesh elements through Bayesian optimization, resulting in a high-accuracy surrogate model. Additionally, it effectively handles the irregular and complex structures of 3D CADs, which differ significantly from the regular and uniform pixel structures of 2D images typically used in deep learning. Experimental results demonstrate that the quality of the mesh significantly impacts the prediction accuracy of the surrogate model, with an optimally sized mesh achieving superior performance. We compare the performance of models based on various 3D representations such as voxel, point cloud, and graph, and evaluate the computational costs of Monte Carlo simulation and Bayesian optimization methods to find the optimal mesh size. We anticipate that our proposed framework has the potential to be applied to mesh-based simulations across various engineering fields, leveraging physics-based information commonly used in computer-aided engineering.</td>
                <td>Submitted 4 June, 2024; </td>
                <td>17 pages, 8 figures, 3 tables</td>
            </tr>
        
            <tr class="item">
                
                <td><a href="https://arxiv.org/abs/2406.01979">arXiv:2406.01979</a></td>
                <td>-Cut Complexes of Squared Cycle Graphs</td>
                <td>Pratiksha Chauhan, Samir Shukla, Kumar Vinayak</td>
                <td>For a positive integer $k$, the $k$-cut complex of a graph $G$ is the simplicial complex whose facets are the $(|V(G)|-k)$-subsets $σ$ of the vertex set $V(G)$ of $G$ such that the induced subgraph of $G$ on $V(G) \setminus σ$ is disconnected. These complexes first appeared in the master thesis of Denker and were further studied by Bayer et al.\ in [Topology of cut complexes of graphs, SIAM Journal on Discrete Mathematics, 2024]. In the same article, Bayer et al.\ conjectured that for $k \geq 3$, the $k$-cut complexes of squared cycle graphs are shellable. Moreover, they also conjectured about the Betti numbers of these complexes when $k=3$. In this article, we prove these conjectures for $k=3$.</td>
                <td>Submitted 4 June, 2024; </td>
                <td>23 pages</td>
            </tr>
        
            <tr class="item">
                
                <td><a href="https://arxiv.org/abs/2406.01977">arXiv:2406.01977</a></td>
                <td>What Improves the Generalization of Graph Transformers? A Theoretical Dive into the Self-attention and Positional Encoding</td>
                <td>Hongkang Li, Meng Wang, Tengfei Ma, Sijia Liu, Zaixi Zhang, Pin-Yu Chen</td>
                <td>Graph Transformers, which incorporate self-attention and positional encoding, have recently emerged as a powerful architecture for various graph learning tasks. Despite their impressive performance, the complex non-convex interactions across layers and the recursive graph structure have made it challenging to establish a theoretical foundation for learning and generalization. This study introduces the first theoretical investigation of a shallow Graph Transformer for semi-supervised node classification, comprising a self-attention layer with relative positional encoding and a two-layer perceptron. Focusing on a graph data model with discriminative nodes that determine node labels and non-discriminative nodes that are class-irrelevant, we characterize the sample complexity required to achieve a desirable generalization error by training with stochastic gradient descent (SGD). This paper provides the quantitative characterization of the sample complexity and number of iterations for convergence dependent on the fraction of discriminative nodes, the dominant patterns, and the initial model errors. Furthermore, we demonstrate that self-attention and positional encoding enhance generalization by making the attention map sparse and promoting the core neighborhood during training, which explains the superior feature representation of Graph Transformers. Our theoretical results are supported by empirical experiments on synthetic and real-world benchmarks.</td>
                <td>Submitted 4 June, 2024; </td>
                <td>ICML 2024</td>
            </tr>
        
            <tr class="item">
                
                <td><a href="https://arxiv.org/abs/2406.01969">arXiv:2406.01969</a></td>
                <td>Multiway Multislice PHATE: Visualizing Hidden Dynamics of RNNs through Training</td>
                <td>Jiancheng Xie, Lou C. Kohler Voinov, Noga Mudrik, Gal Mishne, Adam Charles</td>
                <td>Recurrent neural networks (RNNs) are a widely used tool for sequential data analysis, however, they are still often seen as black boxes of computation. Understanding the functional principles of these networks is critical to developing ideal model architectures and optimization strategies. Previous studies typically only emphasize the network representation post-training, overlooking their evolution process throughout training. Here, we present Multiway Multislice PHATE (MM-PHATE), a novel method for visualizing the evolution of RNNs&#39; hidden states. MM-PHATE is a graph-based embedding using structured kernels across the multiple dimensions spanned by RNNs: time, training epoch, and units. We demonstrate on various datasets that MM-PHATE uniquely preserves hidden representation community structure among units and identifies information processing and compression phases during training. The embedding allows users to look under the hood of RNNs across training and provides an intuitive and comprehensive strategy to understanding the network&#39;s internal dynamics and draw conclusions, e.g., on why and how one model outperforms another or how a specific architecture might impact an RNN&#39;s learning ability.</td>
                <td>Submitted 4 June, 2024; </td>
                <td></td>
            </tr>
        
            <tr class="item">
                
                <td><a href="https://arxiv.org/abs/2406.01934">arXiv:2406.01934</a></td>
                <td>Optimal Transport Guided Correlation Assignment for Multimodal Entity Linking</td>
                <td>Zefeng Zhang, Jiawei Sheng, Chuang Zhang, Yunzhi Liang, Wenyuan Zhang, Siqi Wang, Tingwen Liu</td>
                <td>Multimodal Entity Linking (MEL) aims to link ambiguous mentions in multimodal contexts to entities in a multimodal knowledge graph. A pivotal challenge is to fully leverage multi-element correlations between mentions and entities to bridge modality gap and enable fine-grained semantic matching. Existing methods attempt several local correlative mechanisms, relying heavily on the automatically learned attention weights, which may over-concentrate on partial correlations. To mitigate this issue, we formulate the correlation assignment problem as an optimal transport (OT) problem, and propose a novel MEL framework, namely OT-MEL, with OT-guided correlation assignment. Thereby, we exploit the correlation between multimodal features to enhance multimodal fusion, and the correlation between mentions and entities to enhance fine-grained matching. To accelerate model prediction, we further leverage knowledge distillation to transfer OT assignment knowledge to attention mechanism. Experimental results show that our model significantly outperforms previous state-of-the-art baselines and confirm the effectiveness of the OT-guided correlation assignment.</td>
                <td>Submitted 3 June, 2024; </td>
                <td>Findings of ACL 2024</td>
            </tr>
        
            <tr class="item">
                
                <td><a href="https://arxiv.org/abs/2406.01928">arXiv:2406.01928</a></td>
                <td>History-Aware Planning for Risk-free Autonomous Navigation on Unknown Uneven Terrain</td>
                <td>Yinchuan Wang, Nianfei Du, Yongsen Qin, Xiang Zhang, Rui Song, Chaoqun Wang</td>
                <td>It is challenging for the mobile robot to achieve autonomous and mapless navigation in the unknown environment with uneven terrain. In this study, we present a layered and systematic pipeline. At the local level, we maintain a tree structure that is dynamically extended with the navigation. This structure unifies the planning with the terrain identification. Besides, it contributes to explicitly identifying the hazardous areas on uneven terrain. In particular, certain nodes of the tree are consistently kept to form a sparse graph at the global level, which records the history of the exploration. A series of subgoals that can be obtained in the tree and the graph are utilized for leading the navigation. To determine a subgoal, we develop an evaluation method whose input elements can be efficiently obtained on the layered structure. We conduct both simulation and real-world experiments to evaluate the developed method and its key modules. The experimental results demonstrate the effectiveness and efficiency of our method. The robot can travel through the unknown uneven region safely and reach the target rapidly without a preconstructed map.</td>
                <td>Submitted 3 June, 2024; </td>
                <td>This paper has been accepted by 2024 IEEE International Conference on Robotics and Automation (ICRA 2024)</td>
            </tr>
        
            <tr class="item">
                
                <td><a href="https://arxiv.org/abs/2406.01910">arXiv:2406.01910</a></td>
                <td>Convergence Properties of the Asynchronous Maximum Model</td>
                <td>John Larkin</td>
                <td>Let $G = (V,E)$ be a connected directed graph on $n$ vertices. Assign values from the set $\{1,2,\dots,n\}$ to the vertices of $G$ and update the values according to the following rule: uniformly at random choose a vertex and update its value to the maximum of the values in its neighbourhood. The value at this vertex can potentially decrease. This random process is called the asynchronous maximum model. Repeating this process we show that for a strongly connected directed graph eventually all vertices have the same value and the model is said to have \textit{converged}. In the undirected case the expected convergence time is shown to be asymptotically (as $n\to \infty$) in $Ω(n\log n)$ and $O(n^2)$ and these bounds are tight. We further characterise the convergence time in $O(\frac{n}φ\log n)$ where $φ$ is the vertex expansion of $G$. This provides a better upper bound for a large class of graphs. Further, we show the number of rounds until convergence is in $O((\frac{n}φ\log n)g(n))$ with high probability, where $g(n)$ satisfies $\frac{1}{g^2(n)} \to 0$ as $n \to \infty$. For a strongly connected directed graph the convergence time is shown to be in $O(nb^2 &#43; \frac{n}{φ&#39;}\log n)$ where $b$ is a parameter measuring directed cycle length and $φ&#39;$ is a parameter measuring vertex expansion.</td>
                <td>Submitted 3 June, 2024; </td>
                <td></td>
            </tr>
        
            <tr class="item">
                
                <td><a href="https://arxiv.org/abs/2406.01908">arXiv:2406.01908</a></td>
                <td>PDHG-Unrolled Learning-to-Optimize Method for Large-Scale Linear Programming</td>
                <td>Bingheng Li, Linxin Yang, Yupeng Chen, Senmiao Wang, Qian Chen, Haitao Mao, Yao Ma, Akang Wang, Tian Ding, Jiliang Tang, Ruoyu Sun</td>
                <td>Solving large-scale linear programming (LP) problems is an important task in various areas such as communication networks, power systems, finance and logistics. Recently, two distinct approaches have emerged to expedite LP solving: (i) First-order methods (FOMs); (ii) Learning to optimize (L2O). In this work, we propose an FOM-unrolled neural network (NN) called PDHG-Net, and propose a two-stage L2O method to solve large-scale LP problems. The new architecture PDHG-Net is designed by unrolling the recently emerged PDHG method into a neural network, combined with channel-expansion techniques borrowed from graph neural networks. We prove that the proposed PDHG-Net can recover PDHG algorithm, thus can approximate optimal solutions of LP instances with a polynomial number of neurons. We propose a two-stage inference approach: first use PDHG-Net to generate an approximate solution, and then apply PDHG algorithm to further improve the solution. Experiments show that our approach can significantly accelerate LP solving, achieving up to a 3$\times$ speedup compared to FOMs for large-scale LP problems.</td>
                <td>Submitted 3 June, 2024; </td>
                <td>Accepted by ICML 2024</td>
            </tr>
        
            <tr class="item">
                
                <td><a href="https://arxiv.org/abs/2406.01899">arXiv:2406.01899</a></td>
                <td>Cross-Domain Graph Data Scaling: A Showcase with Diffusion Models</td>
                <td>Wenzhuo Tang, Haitao Mao, Danial Dervovic, Ivan Brugere, Saumitra Mishra, Yuying Xie, Jiliang Tang</td>
                <td>Models for natural language and images benefit from data scaling behavior: the more data fed into the model, the better they perform. This &#39;better with more&#39; phenomenon enables the effectiveness of large-scale pre-training on vast amounts of data. However, current graph pre-training methods struggle to scale up data due to heterogeneity across graphs. To achieve effective data scaling, we aim to develop a general model that is able to capture diverse data patterns of graphs and can be utilized to adaptively help the downstream tasks. To this end, we propose UniAug, a universal graph structure augmentor built on a diffusion model. We first pre-train a discrete diffusion model on thousands of graphs across domains to learn the graph structural patterns. In the downstream phase, we provide adaptive enhancement by conducting graph structure augmentation with the help of the pre-trained diffusion model via guided generation. By leveraging the pre-trained diffusion model for structure augmentation, we consistently achieve performance improvements across various downstream tasks in a plug-and-play manner. To the best of our knowledge, this study represents the first demonstration of a data-scaling graph structure augmentor on graphs across domains.</td>
                <td>Submitted 3 June, 2024; </td>
                <td></td>
            </tr>
        
            <tr class="item">
                
                <td><a href="https://arxiv.org/abs/2406.01890">arXiv:2406.01890</a></td>
                <td>A Ramsey-type theorem on deficiency</td>
                <td>Jin Sun, Xinmin Hou</td>
                <td>Ramsey&#39;s Theorem states that a graph $G$ has bounded order if and only if $G$ contains no complete graph $K_n$ or empty graph $E_n$ as its induced subgraph. The Gyárfás-Sumner conjecture says that a graph $G$ has bounded chromatic number if and only if it contains no induced subgraph isomorphic to $K_n$ or a tree $T$. The deficiency of a graph is the number of vertices that cannot be covered by a maximum matching. In this paper, we prove a Ramsey type theorem for deficiency, i.e., we characterize all the forbidden induced subgraphs for graphs $G$ with bounded deficiency. As an application, we answer a question proposed by Fujita, Kawarabayashi, Lucchesi, Ota, Plummer and Saito (JCTB, 2006).</td>
                <td>Submitted 3 June, 2024; </td>
                <td>17 pages</td>
            </tr>
        
            <tr class="item">
                
                <td><a href="https://arxiv.org/abs/2406.01880">arXiv:2406.01880</a></td>
                <td>An application of node and edge nonlinear hypergraph centrality to a protein complex hypernetwork</td>
                <td>Sarah Lawson, Diane Donovan, James Lefevre</td>
                <td>The use of graph centrality measures applied to biological networks, such as protein interaction networks, underpins much research into identifying key players within biological processes. This approach however is restricted to dyadic interactions and it is well-known that in many instances interactions are polyadic. In this study we illustrate the merit of using hypergraph centrality applied to a hypernetwork as an alternative. Specifically, we review and propose an extension to a recently introduced node and edge nonlinear hypergraph centrality model which provides mutually dependent node and edge centralities. A Saccharomyces Cerevisiae protein complex hypernetwork is used as an example application with nodes representing proteins and hyperedges representing protein complexes. The resulting rankings of the nodes and edges are considered to see if they provide insight into the essentiality of the proteins and complexes. We find that certain variations of the model predict essentiality more accurately and that the degree-based variation illustrates that the centrality-lethality rule extends to a hypergraph setting. In particular, through exploitation of the models flexibility, we identify small sets of proteins densely populated with essential proteins.</td>
                <td>Submitted 3 June, 2024; </td>
                <td>14 pages, 7 figures</td>
            </tr>
        
            <tr class="item">
                
                <td><a href="https://arxiv.org/abs/2406.01842">arXiv:2406.01842</a></td>
                <td>GraphWeaver: Billion-Scale Cybersecurity Incident Correlation</td>
                <td>Scott Freitas, Amir Gharib</td>
                <td>In the dynamic landscape of large enterprise cybersecurity, accurately and efficiently correlating billions of security alerts into comprehensive incidents is a substantial challenge. Traditional correlation techniques often struggle with maintenance, scaling, and adapting to emerging threats and novel sources of telemetry. We introduce GraphWeaver, an industry-scale framework that shifts the traditional incident correlation process to a data-optimized, geo-distributed graph based approach. GraphWeaver introduces a suite of innovations tailored to handle the complexities of correlating billions of shared evidence alerts across hundreds of thousands of enterprises. Key among these innovations are a geo-distributed database and PySpark analytics engine for large-scale data processing, a minimum spanning tree algorithm to optimize correlation storage, integration of security domain knowledge and threat intelligence, and a human-in-the-loop feedback system to continuously refine key correlation processes and parameters. GraphWeaver is integrated into the Microsoft Defender XDR product and deployed worldwide, handling billions of correlations with a 99% accuracy rate, as confirmed by customer feedback and extensive investigations by security experts. This integration has not only maintained high correlation accuracy but reduces traditional correlation storage requirements by 7.4x. We provide an in-depth overview of the key design and operational features of GraphWeaver, setting a precedent as the first cybersecurity company to openly discuss these critical capabilities at this level of depth.</td>
                <td>Submitted 3 June, 2024; </td>
                <td></td>
            </tr>
        
            <tr class="item">
                
                <td><a href="https://arxiv.org/abs/2406.01823">arXiv:2406.01823</a></td>
                <td>Causal Discovery with Fewer Conditional Independence Tests</td>
                <td>Kirankumar Shiragur, Jiaqi Zhang, Caroline Uhler</td>
                <td>Many questions in science center around the fundamental problem of understanding causal relationships. However, most constraint-based causal discovery algorithms, including the well-celebrated PC algorithm, often incur an exponential number of conditional independence (CI) tests, posing limitations in various applications. Addressing this, our work focuses on characterizing what can be learned about the underlying causal graph with a reduced number of CI tests. We show that it is possible to a learn a coarser representation of the hidden causal graph with a polynomial number of tests. This coarser representation, named Causal Consistent Partition Graph (CCPG), comprises of a partition of the vertices and a directed graph defined over its components. CCPG satisfies consistency of orientations and additional constraints which favor finer partitions. Furthermore, it reduces to the underlying causal graph when the causal graph is identifiable. As a consequence, our results offer the first efficient algorithm for recovering the true causal graph with a polynomial number of tests, in special cases where the causal graph is fully identifiable through observational data and potentially additional interventions.</td>
                <td>Submitted 3 June, 2024; </td>
                <td></td>
            </tr>
        
            <tr class="item">
                
                <td><a href="https://arxiv.org/abs/2406.01808">arXiv:2406.01808</a></td>
                <td>In-Context Learning of Physical Properties: Few-Shot Adaptation to Out-of-Distribution Molecular Graphs</td>
                <td>Grzegorz Kaszuba, Amirhossein D. Naghdi, Dario Massa, Stefanos Papanikolaou, Andrzej Jaszkiewicz, Piotr Sankowski</td>
                <td>Large language models manifest the ability of few-shot adaptation to a sequence of provided examples. This behavior, known as in-context learning, allows for performing nontrivial machine learning tasks during inference only. In this work, we address the question: can we leverage in-context learning to predict out-of-distribution materials properties? However, this would not be possible for structure property prediction tasks unless an effective method is found to pass atomic-level geometric features to the transformer model. To address this problem, we employ a compound model in which GPT-2 acts on the output of geometry-aware graph neural networks to adapt in-context information. To demonstrate our model&#39;s capabilities, we partition the QM9 dataset into sequences of molecules that share a common substructure and use them for in-context learning. This approach significantly improves the performance of the model on out-of-distribution examples, surpassing the one of general graph neural network models.</td>
                <td>Submitted 3 June, 2024; </td>
                <td>12 pages, 4 figures</td>
            </tr>
        
            <tr class="item">
                
                <td><a href="https://arxiv.org/abs/2406.01790">arXiv:2406.01790</a></td>
                <td>The bunkbed conjecture is not robust to generalisation</td>
                <td>Lawrence Hollom</td>
                <td>The bunkbed conjecture, which has featured in the folklore of probability theory since at least 1985, concerns bond percolation on the product graph $G\Box K_2$. We have two copies $G_0$ and $G_1$ of $G$, and if $x^{(0)}$ and $x^{(1)}$ are the copies of a vertex $x\in V(G)$ in $G_0$ and $G_1$ respectively, then edge $x^{(0)}x^{(1)}$ is present. The conjecture states that, for vertices $u,v\in V(G)$, percolation from $u^{(0)}$ to $v^{(0)}$ is at least as likely as percolation from $u^{(0)}$ to $v^{(1)}$. While the conjecture is widely expected to be true, having attracted significant attention, a general proof has not been forthcoming.</td>
                <td>Submitted 3 June, 2024; </td>
                <td>16 pages</td>
            </tr>
        
            <tr class="item">
                
                <td><a href="https://arxiv.org/abs/2406.01759">arXiv:2406.01759</a></td>
                <td>From Latent to Lucid: Transforming Knowledge Graph Embeddings into Interpretable Structures</td>
                <td>Christoph Wehner, Chrysa Iliopoulou, Tarek R. Besold</td>
                <td>This paper introduces a post-hoc explainable AI method tailored for Knowledge Graph Embedding models. These models are essential to Knowledge Graph Completion yet criticized for their opaque, black-box nature. Despite their significant success in capturing the semantics of knowledge graphs through high-dimensional latent representations, their inherent complexity poses substantial challenges to explainability. Unlike existing methods, our approach directly decodes the latent representations encoded by Knowledge Graph Embedding models, leveraging the principle that similar embeddings reflect similar behaviors within the Knowledge Graph. By identifying distinct structures within the subgraph neighborhoods of similarly embedded entities, our method identifies the statistical regularities on which the models rely and translates these insights into human-understandable symbolic rules and facts. This bridges the gap between the abstract representations of Knowledge Graph Embedding models and their predictive outputs, offering clear, interpretable insights. Key contributions include a novel post-hoc explainable AI method for Knowledge Graph Embedding models that provides immediate, faithful explanations without retraining, facilitating real-time application even on large-scale knowledge graphs. The method&#39;s flexibility enables the generation of rule-based, instance-based, and analogy-based explanations, meeting diverse user needs. Extensive evaluations show our approach&#39;s effectiveness in delivering faithful and well-localized explanations, enhancing the transparency and trustworthiness of Knowledge Graph Embedding models.</td>
                <td>Submitted 3 June, 2024; </td>
                <td></td>
            </tr>
        
            <tr class="item">
                
                <td><a href="https://arxiv.org/abs/2406.01743">arXiv:2406.01743</a></td>
                <td>Quantum optimization using a 127-qubit gate-model IBM quantum computer can outperform quantum annealers for nontrivial binary optimization problems</td>
                <td>Natasha Sachdeva, Gavin S. Harnett, Smarak Maity, Samuel Marsh, Yulun Wang, Adam Winick, Ryan Dougherty, Daniel Canuto, You Quan Chong, Michael Hush, Pranav S. Mundada, Christopher D. B. Bentley, Michael J. Biercuk, Yuval Baum</td>
                <td>We introduce a comprehensive quantum solver for binary combinatorial optimization problems on gate-model quantum computers that outperforms any published alternative and consistently delivers correct solutions for problems with up to 127 qubits. We provide an overview of the internal workflow, describing the integration of a customized ansatz and variational parameter update strategy, efficient error suppression in hardware execution, and overhead-free post-processing to correct for bit-flip errors. We benchmark this solver on IBM quantum computers for several classically nontrivial unconstrained binary optimization problems -- the entire optimization is conducted on hardware with no use of classical simulation or prior knowledge of the solution. First, we demonstrate the ability to correctly solve Max-Cut instances for random regular graphs with a variety of densities using up to 120 qubits, where the graph topologies are not matched to device connectivity. Next, we apply the solver to higher-order binary optimization and successfully search for the ground state energy of a 127-qubit spin-glass model with linear, quadratic, and cubic interaction terms. Use of this new quantum solver increases the likelihood of finding the minimum energy by up to $\sim1,500\times$ relative to published results using a DWave annealer, and it can find the correct solution when the annealer fails. Furthermore, for both problem types, the Q-CTRL solver outperforms a heuristic local solver used to indicate the relative difficulty of the problems pursued. Overall, these results represent the largest quantum optimizations successfully solved on hardware to date, and demonstrate the first time a gate-model quantum computer has been able to outperform an annealer for a class of binary optimization problems.</td>
                <td>Submitted 3 June, 2024; </td>
                <td>18 pages, 7 figures</td>
            </tr>
        
            <tr class="item">
                
                <td><a href="https://arxiv.org/abs/2406.01739">arXiv:2406.01739</a></td>
                <td>A Surprisingly Simple Method for Distributed Euclidean-Minimum Spanning Tree / Single Linkage Dendrogram Construction from High Dimensional Embeddings via Distance Decomposition</td>
                <td>Richard Lettich</td>
                <td>We introduce a decomposition method for the distributed calculation of exact Euclidean Minimum Spanning Trees in high dimensions (where sub-quadratic algorithms are not effective), or more generalized geometric-minimum spanning trees of complete graphs, where for each vertex $v\in V$ in the graph $G=(V,E)$ is represented by a vector in $\vec{v}\in \mathbb{R}^n$, and each for any edge, the the weight of the edge in the graph is given by a symmetric binary `distance&#39; function between the representative vectors $w(\{x,y\}) = d(\vec{x},\vec{y})$. This is motivated by the task of clustering high dimensional embeddings produced by neural networks, where low-dimensional algorithms are ineffective; such geometric-minimum spanning trees find applications as a subroutine in the construction of single linkage dendrograms, as the two structures can be converted between each other efficiently.</td>
                <td>Submitted 3 June, 2024; </td>
                <td>2 pages</td>
            </tr>
        
    </table>
</body>
</html>

<style>
    table {
        width: 100%;
        border-collapse: collapse;
    }

    table, th, td {
        border: 1px solid black;
    }

    th, td {
        padding: 8px;
        text-align: left;
    }

     
    tr:nth-child(odd) {
        background-color: #f2f2f2;
    }

     
    tr:nth-child(even) {
        background-color: #ffffff;
    }

    th {
        background-color: #4CAF50;
        color: white;
    }
</style>